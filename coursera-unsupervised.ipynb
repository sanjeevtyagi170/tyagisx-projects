{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing Libraries","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-05T14:57:32.539271Z","iopub.execute_input":"2023-05-05T14:57:32.539696Z","iopub.status.idle":"2023-05-05T14:57:32.546928Z","shell.execute_reply.started":"2023-05-05T14:57:32.539662Z","shell.execute_reply":"2023-05-05T14:57:32.545809Z"}}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os, datetime\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler,LabelEncoder,StandardScaler\nfrom sklearn.decomposition import PCA, non_negative_factorization\nfrom sklearn.model_selection import StratifiedShuffleSplit, train_test_split\nfrom sklearn.cluster import KMeans, AgglomerativeClustering,MeanShift,DBSCAN,Birch\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, f1_score, roc_auc_score,recall_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n## About Dataset\nThis dataset is taken from **KAGGLE**. \n## Context\n## Problem Statement\n\nCustomer Personality Analysis is a detailed analysis of a company’s ideal customers. It helps a business to better understand its customers and makes it easier for them to modify products according to the specific needs, behaviors and concerns of different types of customers.\n\nCustomer personality analysis helps a business to modify its product based on its target customers from different types of customer segments. For example, instead of spending money to market a new product to every customer in the company’s database, a company can analyze which customer segment is most likely to buy the product and then market the product only on that particular segment.\n\n#### Link of the dataset \nhttps://www.kaggle.com/datasets/imakash3011/customer-personality-analysis?select=marketing_campaign.csv\n\n### Content\n### Attributes\n\n#### People\n\n* ID: Customer's unique identifier\n* Year_Birth: Customer's birth year\n* Education: Customer's education level\n* Marital_Status: Customer's marital status\n* Income: Customer's yearly household income\n* Kidhome: Number of children in customer's household\n* Teenhome: Number of teenagers in customer's household\n* Dt_Customer: Date of customer's enrollment with the company\n* Recency: Number of days since customer's last purchase\n* Complain: 1 if the customer complained in the last 2 years, 0 otherwise\n\n#### Products\n\n* MntWines: Amount spent on wine in last 2 years\n* MntFruits: Amount spent on fruits in last 2 years\n* MntMeatProducts: Amount spent on meat in last 2 years\n* MntFishProducts: Amount spent on fish in last 2 years\n* MntSweetProducts: Amount spent on sweets in last 2 years\n* MntGoldProds: Amount spent on gold in last 2 years\n\n#### Promotion\n\n* NumDealsPurchases: Number of purchases made with a discount\n* AcceptedCmp1: 1 if customer accepted the offer in the 1st campaign, 0 otherwise\n* AcceptedCmp2: 1 if customer accepted the offer in the 2nd campaign, 0 otherwise\n* AcceptedCmp3: 1 if customer accepted the offer in the 3rd campaign, 0 otherwise\n* AcceptedCmp4: 1 if customer accepted the offer in the 4th campaign, 0 otherwise\n* AcceptedCmp5: 1 if customer accepted the offer in the 5th campaign, 0 otherwise\n* Response: 1 if customer accepted the offer in the last campaign, 0 otherwise\n\n#### Place\n\n* NumWebPurchases: Number of purchases made through the company’s website\n* NumCatalogPurchases: Number of purchases made using a catalogue\n* NumStorePurchases: Number of purchases made directly in stores\n* NumWebVisitsMonth: Number of visits to company’s website in the last month\n\n#### Purpose of this exercise\n* Need to perform clustering to summarize customer segments.\n\n","metadata":{}},{"cell_type":"markdown","source":"# Main objective\nIn this project, We will be performing unsupervised clustering on the customer's data from a groceries firm's database. Customer segmentation is the practice of separating customers into groups that reflect similarities among customers in each cluster. We will divide the customers into segments to understand the behaviour of customers as a group to the business. To modify products according to distinct needs of the customers. \n\n# Stakeholders\nBy this analysis, our stakeholders will get to know customer behaviour, who are our most loyal customers and customer who are visting the store less.\nIt also helps the business to cater to the concerns of different types of customers.","metadata":{}},{"cell_type":"code","source":"data=pd.read_csv('/kaggle/input/customer-personality-analysis/marketing_campaign.csv', sep=\"\\t\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_columns', None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Cleaning and Exploration","metadata":{"execution":{"iopub.status.busy":"2023-04-30T08:54:23.325396Z","iopub.execute_input":"2023-04-30T08:54:23.325843Z","iopub.status.idle":"2023-04-30T08:54:23.330982Z","shell.execute_reply.started":"2023-04-30T08:54:23.325800Z","shell.execute_reply":"2023-04-30T08:54:23.329809Z"}}},{"cell_type":"code","source":"data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Categorical Variables encoded\nlabels = LabelEncoder()\ndata.Marital_Status=labels.fit_transform(data.Marital_Status)\ndata.Education=labels.fit_transform(data.Education)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# year_birth converted to age\ndata[\"age\"]=datetime.datetime.today().year-data.Year_Birth\ndata=data.drop(\"Year_Birth\",axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# renaming column names for more meaning\ndata.columns=['ID', 'education', 'marital_status', 'income', 'kidhome',\n       'teenhome', 'cust_enrol_dt', 'recency', 'amt_spent_wine_last_2_yr', 'amt_spent_fruits_last_2_yr',\n       'amt_spent_meat_last_2_yr', 'amt_spent_fish_last_2_yr', 'amt_spent_sweet_last_2_yr',\n       'amt_spent_gold_last_2_yr', 'num_deals_purchases', 'num_web_purchases',\n       'num_catalog_purchases','num_store_purchases','num_web_visits_month',\n       'accepted_cmp3', 'accepted_cmp4', 'accepted_cmp5', 'accepted_cmp1',\n       'accepted_cmp2', 'complain', 'Z_cost_contact', 'Z_revenue', 'response',\n       'age']\n# dt_customer column data type corrected\ndata.cust_enrol_dt=pd.to_datetime(data.cust_enrol_dt)\n# data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['amount_spent']=data[['amt_spent_wine_last_2_yr', 'amt_spent_fruits_last_2_yr','amt_spent_meat_last_2_yr', 'amt_spent_fish_last_2_yr',\n                           'amt_spent_sweet_last_2_yr','amt_spent_gold_last_2_yr']].sum(axis=1)\ndata=data.drop(['amt_spent_wine_last_2_yr', 'amt_spent_fruits_last_2_yr','amt_spent_meat_last_2_yr', 'amt_spent_fish_last_2_yr',\n                           'amt_spent_sweet_last_2_yr','amt_spent_gold_last_2_yr'],axis=1)\ncols=data.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now we can use this dataset for clustering as none of the columns are categorical\ndata.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Finding the unique values in every column\npd.DataFrame([[i, len(data[i].unique())] for i in data.columns],\n             columns=['Variable', 'Unique Values']).set_index('Variable')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check for missing values in dataset  - Missing values are handled using imputation\n# All the variables are numeric in nature\ndata.income=data.income.fillna(np.mean(data.income))\ndata.isna().sum()\ndata.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check for duplicate customer ids # no duplicate ids found\nassert data.ID.duplicated(keep=False).count()==data.shape[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# From the Histogram it is clear that there are some people whose age is greated than 120 on an average people live around 100 years\n# So removing age values which are greater than 100\ndata=data[data.age<=100]\nf, (ax1, ax2) = plt.subplots(1, 2)\ndata.age.hist(ax=ax1)\nax1.set_title(\"Histogram\")\nsns.boxplot(data.age,ax=ax2)\nax2.set_title(\"Boxplot\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking the correlation between varibles\nplt.figure(figsize = (12, 10))\nsns.heatmap(data.corr(), annot = True, linewidths=0,fmt='.2f',annot_kws={\"size\": 8})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Standardize the data","metadata":{}},{"cell_type":"code","source":"# Features to be considered\nX=data.drop(['cust_enrol_dt','ID'],axis=1)\nmms=MinMaxScaler()\ntransformed_data=mms.fit_transform(X,)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaled_data=pd.DataFrame(transformed_data)\nscaled_data.columns=set(cols)-{'cust_enrol_dt','ID'}\nscaled_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Training and Predictions","metadata":{"execution":{"iopub.status.busy":"2023-04-30T09:03:03.344244Z","iopub.execute_input":"2023-04-30T09:03:03.345502Z","iopub.status.idle":"2023-04-30T09:03:03.350793Z","shell.execute_reply.started":"2023-04-30T09:03:03.345428Z","shell.execute_reply":"2023-04-30T09:03:03.349341Z"}}},{"cell_type":"markdown","source":"# Kmeans","metadata":{}},{"cell_type":"code","source":"# From the plot it is okay to create 4-5 clusters\nkmeans=KMeans(n_clusters=5)\nkmeans.fit(transformed_data)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot to find optimal K\n\ninertia = []\nlist_num_clusters = list(range(1,15))\nfor num_clusters in list_num_clusters:\n    km = KMeans(n_clusters=num_clusters)\n    km.fit(transformed_data)\n    inertia.append(km.inertia_)\n    \nplt.plot(list_num_clusters,inertia)\nplt.scatter(list_num_clusters,inertia)\nplt.xlabel('Number of Clusters')\nplt.ylabel('Inertia');\n### END SOLUTION","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kmeans_labels=kmeans.labels_\nset(kmeans.labels_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Agglomerative clustering","metadata":{}},{"cell_type":"code","source":"agc=AgglomerativeClustering(n_clusters=5)\nagc.fit(transformed_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"agc_labels=agc.labels_\nset(agc.labels_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# BIRCH","metadata":{}},{"cell_type":"code","source":"birch=Birch(n_clusters=5)\nbirch.fit(transformed_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 4 clusters generated\nbirch_labels=birch.labels_\nset(birch.labels_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Aggregate all the clusters labels into original dataframe with the help of mode and analyze clusters","metadata":{}},{"cell_type":"code","source":"data['kmeans']=kmeans_labels\ndata['agc']=agc_labels\ndata['birch']=birch_labels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_clusters=data[[\"kmeans\",\"agc\",\"birch\"]]\ndata['voting_labels']=df_clusters.mode(axis=1)[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Analyze Clusters","metadata":{"execution":{"iopub.status.busy":"2023-05-05T17:10:44.397037Z","iopub.execute_input":"2023-05-05T17:10:44.397405Z","iopub.status.idle":"2023-05-05T17:10:44.449004Z","shell.execute_reply.started":"2023-05-05T17:10:44.397374Z","shell.execute_reply":"2023-05-05T17:10:44.447824Z"}}},{"cell_type":"code","source":"df_c1=data[data.voting_labels==1]\ndf_c1.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_c2=data[data.voting_labels==2]\ndf_c2.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_c3=data[data.voting_labels==3]\ndf_c3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_c4=data[data.voting_labels==4]\ndf_c4","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_c5=data[data.voting_labels==0]\ndf_c5.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Summary and key insights\n\nI have created 5 models logistic, KNN, decision tree, random forests and gradient boosting and used five different validation metrices. below is the summary of all provided how they have performed.\nAll the models are trained on same training sets and tested on same test sets. Also, almost all of the models used same parameters.\nFrom the **confusion matrix** it is evident that **Logistic Regression** performed very badly with **0 precision and recall**.\nKNN and Decision Tree model gave some edge as precision, recall and f1 scores starts to improve in these two models by decreasing some accuracy\nI think gradient boosting method have performed very well as compared to other models with highest accuracy,Precision, recall and highest f1-score.\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-30T09:39:40.054599Z","iopub.execute_input":"2023-04-30T09:39:40.054948Z","iopub.status.idle":"2023-04-30T09:39:40.071838Z","shell.execute_reply.started":"2023-04-30T09:39:40.054908Z","shell.execute_reply":"2023-04-30T09:39:40.070337Z"}}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Importance","metadata":{"execution":{"iopub.status.busy":"2023-04-30T10:39:31.815607Z","iopub.execute_input":"2023-04-30T10:39:31.816109Z","iopub.status.idle":"2023-04-30T10:39:31.824349Z","shell.execute_reply.started":"2023-04-30T10:39:31.816063Z","shell.execute_reply":"2023-04-30T10:39:31.822358Z"}}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Suggestions and next steps for revisiting the model\n\nWe could further optimize these models\n1. Using **GridSearchCV** that will find the best parameters for every model.\n2. Using Sampling because data is **unbalanced**, so we can also look from that angle also to increase the accuracy of the model.\n3. We could also change our model based on the **inputs received from our stakeholders** about the business.\n4. We could also use XGboost model","metadata":{"execution":{"iopub.status.busy":"2023-04-30T09:55:01.627295Z","iopub.execute_input":"2023-04-30T09:55:01.628508Z","iopub.status.idle":"2023-04-30T09:55:01.635770Z","shell.execute_reply.started":"2023-04-30T09:55:01.628463Z","shell.execute_reply":"2023-04-30T09:55:01.633761Z"}}},{"cell_type":"markdown","source":"","metadata":{}}]}